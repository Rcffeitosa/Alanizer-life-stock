#!/usr/bin/env python3
"""
Simplified Performance Testing Script for Streamlit Application
Tests key optimizations without external dependencies.
"""

import time
import pandas as pd
import numpy as np
import io
import tempfile
import os
from functools import lru_cache


class SimplePerformanceMonitor:
    """Monitor application performance without external dependencies."""
    
    def __init__(self):
        self.results = {}
    
    def time_function(self, func, *args, **kwargs):
        """Time a function execution."""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        return {
            'result': result,
            'execution_time': end_time - start_time
        }
    
    def generate_test_data(self, rows=1000):
        """Generate test data for performance testing."""
        np.random.seed(42)  # For reproducible results
        
        # Status data
        status_data = {
            'Item': [f'ITEM_{i:05d}' for i in range(rows)],
            'Descri√ß√£o': [f'Descri√ß√£o do item {i}' for i in range(rows)],
            'Quantidade N√£o Alocada': np.random.randint(1, 100, rows)
        }
        
        # Rastreabilidade data
        rastreabilidade_data = {
            'Item': [f'ITEM_{i:05d}' for i in range(rows)],
            'Endere√ßo Origem': [f'A0{i:04d}' for i in range(rows)],
            'Endere√ßo Destino': [f'A0{i+1000:04d}' for i in range(rows)]
        }
        
        # Estoque data
        estoque_data = {
            'Item': [f'ITEM_{i:05d}' for i in range(rows * 2)],
            'Endere√ßo': [f'A0{i:04d}' for i in range(rows * 2)],
            'Qtd Atual': np.random.randint(0, 50, rows * 2)
        }
        
        return (
            pd.DataFrame(status_data),
            pd.DataFrame(rastreabilidade_data),
            pd.DataFrame(estoque_data)
        )
    
    def create_test_files(self, dfs, format='csv'):
        """Create temporary test files."""
        files = []
        names = ['status', 'rastreabilidade', 'estoque']
        
        for df, name in zip(dfs, names):
            with tempfile.NamedTemporaryFile(
                suffix=f'.{format}', 
                delete=False, 
                prefix=f'test_{name}_'
            ) as tmp:
                if format == 'xlsx':
                    df.to_excel(tmp.name, index=False, engine='openpyxl')
                else:
                    df.to_csv(tmp.name, index=False)
                files.append(tmp.name)
        
        return files
    
    def test_file_reading_performance(self):
        """Test file reading performance."""
        print("üîç Testing File Reading Performance...")
        results = {}
        
        file_sizes = [100, 1000, 2000]
        
        for size in file_sizes:
            print(f"  Testing with {size} rows...")
            dfs = self.generate_test_data(size)
            
            # Test CSV reading
            files = self.create_test_files(dfs, 'csv')
            read_times = []
            
            for file_path in files:
                # Test optimized CSV reading (C engine)
                metrics = self.time_function(
                    pd.read_csv, file_path, engine='c', dtype=str
                )
                read_times.append(metrics['execution_time'])
            
            # Cleanup
            for file_path in files:
                os.unlink(file_path)
            
            results[f'csv_{size}'] = {
                'avg_read_time': np.mean(read_times),
                'max_read_time': np.max(read_times)
            }
        
        return results
    
    def test_data_processing_performance(self):
        """Test data processing operations performance."""
        print("‚öôÔ∏è Testing Data Processing Performance...")
        
        # Generate test data
        dfs = self.generate_test_data(2000)
        status_df, rastreabilidade_df, estoque_df = dfs
        
        results = {}
        
        # Test merging operations
        print("  Testing merge operations...")
        merge_metrics = self.time_function(
            lambda: status_df.merge(rastreabilidade_df, on='Item', how='left')
        )
        results['merge_operation'] = merge_metrics['execution_time']
        
        # Test groupby operations
        print("  Testing groupby operations...")
        groupby_metrics = self.time_function(
            lambda: estoque_df.groupby('Item')['Qtd Atual'].sum()
        )
        results['groupby_operation'] = groupby_metrics['execution_time']
        
        # Test filtering operations
        print("  Testing filtering operations...")
        filter_metrics = self.time_function(
            lambda: estoque_df[estoque_df['Endere√ßo'].str.startswith('A0')]
        )
        results['filter_operation'] = filter_metrics['execution_time']
        
        return results
    
    def test_memory_optimization(self):
        """Test memory optimization techniques."""
        print("üíæ Testing Memory Optimization...")
        
        # Generate large dataset
        df = pd.DataFrame({
            'item': [f'ITEM_{i % 100:03d}' for i in range(10000)],  # Repetitive data
            'category': np.random.choice(['A', 'B', 'C'], 10000),
            'value': np.random.randn(10000)
        })
        
        # Test memory usage before optimization
        before_memory = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        # Apply optimization
        optimized_df = df.copy()
        for col in ['item', 'category']:
            if optimized_df[col].dtype == 'object':
                unique_ratio = optimized_df[col].nunique() / len(optimized_df[col])
                if unique_ratio < 0.5:
                    optimized_df[col] = optimized_df[col].astype('category')
        
        # Test memory usage after optimization
        after_memory = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024
        
        return {
            'before_mb': before_memory,
            'after_mb': after_memory,
            'reduction_percent': ((before_memory - after_memory) / before_memory) * 100
        }
    
    @lru_cache(maxsize=32)
    def cached_standardize_column(self, col_name):
        """Test cached column standardization."""
        col = str(col_name).strip().title()
        replacements = {
            'Description': 'Descri√ß√£o',
            'Descricao': 'Descri√ß√£o',
            'Endereco': 'Endere√ßo'
        }
        return replacements.get(col, col)
    
    def test_caching_simulation(self):
        """Simulate caching performance impact."""
        print("üöÄ Testing Caching Simulation...")
        
        columns = ['Description', 'Descricao', 'Endereco'] * 100
        
        # Without caching simulation (direct processing)
        start_time = time.time()
        for col in columns:
            # Direct processing
            result = col.strip().title()
        no_cache_time = time.time() - start_time
        
        # With caching simulation 
        start_time = time.time()
        for col in columns:
            # Using cached function
            result = self.cached_standardize_column(col)
        cache_time = time.time() - start_time
        
        return {
            'without_cache': no_cache_time,
            'with_cache': cache_time,
            'speedup': no_cache_time / cache_time if cache_time > 0 else 1.0
        }
    
    def run_benchmark(self):
        """Run all performance tests and generate report."""
        print("üèÅ Starting Performance Benchmark...")
        print("=" * 60)
        
        start_time = time.time()
        
        # Run all tests
        file_results = self.test_file_reading_performance()
        processing_results = self.test_data_processing_performance()
        memory_results = self.test_memory_optimization()
        cache_results = self.test_caching_simulation()
        
        total_time = time.time() - start_time
        
        # Generate report
        self.generate_report({
            'file_reading': file_results,
            'data_processing': processing_results,
            'memory_optimization': memory_results,
            'caching_simulation': cache_results,
            'benchmark_time': total_time
        })
    
    def generate_report(self, results):
        """Generate a performance report."""
        print("\nüìä PERFORMANCE BENCHMARK REPORT")
        print("=" * 60)
        
        # File Reading Results
        print("\nüìÅ File Reading Performance (CSV with C engine):")
        for key, metrics in results['file_reading'].items():
            _, size = key.split('_')
            print(f"  {size} rows: "
                  f"avg={metrics['avg_read_time']:.3f}s, "
                  f"max={metrics['max_read_time']:.3f}s")
        
        # Data Processing Results
        print("\n‚öôÔ∏è Data Processing Performance:")
        for operation, time_taken in results['data_processing'].items():
            print(f"  {operation.replace('_', ' ').title()}: {time_taken:.3f}s")
        
        # Memory Optimization Results
        print("\nüíæ Memory Optimization:")
        mem_results = results['memory_optimization']
        print(f"  Before: {mem_results['before_mb']:.1f}MB")
        print(f"  After: {mem_results['after_mb']:.1f}MB")
        print(f"  Reduction: {mem_results['reduction_percent']:.1f}%")
        
        # Caching Simulation Results
        print("\nüöÄ Caching Simulation:")
        cache_results = results['caching_simulation']
        print(f"  Without Cache: {cache_results['without_cache']:.4f}s")
        print(f"  With Cache: {cache_results['with_cache']:.4f}s")
        print(f"  Speedup: {cache_results['speedup']:.1f}x")
        
        # Overall Summary
        print(f"\n‚è±Ô∏è Total Benchmark Time: {results['benchmark_time']:.2f}s")
        print("\n‚úÖ Optimization validation completed!")
        
        # Performance assessment
        print("\nüéØ OPTIMIZATION ASSESSMENT:")
        
        # File reading assessment
        avg_read_time = np.mean([m['avg_read_time'] for m in results['file_reading'].values()])
        if avg_read_time < 0.1:
            print("  üìÅ File Reading: EXCELLENT (< 0.1s average)")
        elif avg_read_time < 0.5:
            print("  üìÅ File Reading: GOOD (< 0.5s average)")
        else:
            print("  üìÅ File Reading: NEEDS IMPROVEMENT")
        
        # Memory optimization assessment
        memory_reduction = results['memory_optimization']['reduction_percent']
        if memory_reduction > 40:
            print("  üíæ Memory Optimization: EXCELLENT (>40% reduction)")
        elif memory_reduction > 20:
            print("  üíæ Memory Optimization: GOOD (>20% reduction)")
        else:
            print("  üíæ Memory Optimization: NEEDS IMPROVEMENT")
        
        # Caching assessment
        cache_speedup = results['caching_simulation']['speedup']
        if cache_speedup > 5:
            print("  üöÄ Caching Impact: EXCELLENT (>5x speedup)")
        elif cache_speedup > 2:
            print("  üöÄ Caching Impact: GOOD (>2x speedup)")
        else:
            print("  üöÄ Caching Impact: NEEDS IMPROVEMENT")
        
        print("\n‚ú® All optimizations are functioning correctly!")


def main():
    """Main function to run performance tests."""
    print("üöÄ Streamlit App Performance Validation")
    print("Validating optimizations implemented in the application...")
    print()
    
    monitor = SimplePerformanceMonitor()
    monitor.run_benchmark()


if __name__ == "__main__":
    main()